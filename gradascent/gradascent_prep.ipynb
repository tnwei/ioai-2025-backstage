{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07034b99-1afe-4d08-a78e-14293c47fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flag{Welcome_to_IOAI}, xor key=ioai\n",
    "#lfkmq]ofiegoU~eUCEKCw <- encoded hash to be given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6c2a75-b27a-42b8-a3a1-c32d2ca0b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.3.0+cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bde449f-a8df-4b0f-84fd-548167d6465e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67988fbe-7b6f-4944-ac5a-1c19fa5a3e33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fabd069a790>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23b8db39-2e55-42e4-a28e-39b69f867450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i_data = torch.zeros((26,), dtype=torch.float32)\n",
    "i_data[8] = 100 #correct alphabet index will be as large as possible, the rest 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4cfba2a-5758-402c-8e19-c062651966db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ANN - i\n",
    "\n",
    "class SimpleANN(nn.Module):\n",
    "    def __init__(self, input_size=26, hidden_size=64, output_size=50):\n",
    "        super(SimpleANN, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Input to hidden layer\n",
    "        self.relu = nn.ReLU()                         # Activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # Hidden layer to output\n",
    "\n",
    "        # Initialize weights with a specific random seed\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Initialize weights with random values\n",
    "        nn.init.normal_(self.fc1.weight, mean=0.0, std=0.02)  # Normal distribution for fc1\n",
    "        nn.init.normal_(self.fc2.weight, mean=0.0, std=0.02)  # Normal distribution for fc2\n",
    "\n",
    "        # Initialize biases to zero (optional)\n",
    "        nn.init.constant_(self.fc1.bias, 0.0)\n",
    "        nn.init.constant_(self.fc2.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        x = self.fc1(x)  # Input layer to hidden layer\n",
    "        x = self.relu(x) # Activation function\n",
    "        x = self.fc2(x)  # Hidden layer to output layer\n",
    "        return x\n",
    "\n",
    "model = SimpleANN()\n",
    "i_output = model(i_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43305566-70ef-4fa8-9624-d53ed5ec98bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_data = torch.zeros((1, 1, 26), dtype=torch.float32)\n",
    "o_data[0][0][14] = 100 #correct alphabet index will be as large as possible, the rest 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c84f0754-65b7-491e-83e3-24b23dfc6d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN - o\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size=26, hidden_size=64, output_size=50, num_layers=1):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        \n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer for the output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Initialize weights with a specific random seed\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Custom weight initialization for reproducibility\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # Initialize RNN weights\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.kaiming_normal_(param, mode='fan_out', nonlinearity='relu')\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "        # Initialize fully connected layer weights\n",
    "        nn.init.xavier_uniform_(self.fc.weight)  # Xavier initialization for fc layer\n",
    "        nn.init.constant_(self.fc.bias, 0.0)     # Set biases to zero\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Forward pass through RNN\n",
    "        out, hidden = self.rnn(x, hidden)  # RNN layer\n",
    "        out = self.fc(out[:, -1, :])       # Fully connected layer (take last time step's output)\n",
    "        return out, hidden\n",
    "\n",
    "hidden_size = 64  # Hidden state size\n",
    "batch_size = 1        # Batch size\n",
    "num_layers = 1        # Number of RNN layers\n",
    "\n",
    "model = SimpleRNN()\n",
    "hidden = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "o_output, _ = model(o_data, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d3e4c7-e90b-4ba3-8b39-8948905738aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_data = torch.zeros((26,), dtype=torch.float32)\n",
    "a_data[0] = 100 #correct alphabet index will be as large as possible, the rest 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22bfc16a-c054-48d2-aabb-d648c1342a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP - a\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size=26, hidden_size=64, output_size=50):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Input to hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size) # Hidden layer to hidden layer\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)  # Hidden layer to output layer\n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "        \n",
    "        # Initialize weights with a specific random seed\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Custom weight initialization for reproducibility\n",
    "        torch.manual_seed(42)  # Optional: redundant if already set globally\n",
    "\n",
    "        # Initialize fully connected layer weights with Xavier (Glorot) initialization\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        \n",
    "        # Initialize biases to zero for fc layers\n",
    "        nn.init.constant_(self.fc1.bias, 0.0)\n",
    "        nn.init.constant_(self.fc2.bias, 0.0)\n",
    "        nn.init.constant_(self.fc3.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        x = self.fc1(x)        # Input layer to first hidden layer\n",
    "        x = self.relu(x)       # ReLU activation\n",
    "        x = self.fc2(x)        # First hidden layer to second hidden layer\n",
    "        x = self.relu(x)       # ReLU activation\n",
    "        x = self.fc3(x)        # Second hidden layer to output layer\n",
    "        return x\n",
    "\n",
    "model = SimpleMLP()\n",
    "a_output = model(a_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad52e98b-42f6-4601-aa25-63d51d5ff2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "i2_data = torch.zeros((26,), dtype=torch.float32)\n",
    "i2_data[8] = 100 #correct alphabet index will be as large as possible, the rest 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce83acc3-11f3-4de3-8ed6-6b9ce27ad40a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#KAN - i\n",
    "class SimpleKAN(nn.Module):\n",
    "    def __init__(self, input_size=26, hidden_size=64, output_size=50):\n",
    "        super(SimpleKAN, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Input to hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size) # Hidden layer to hidden layer\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)  # Hidden layer to output layer\n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "\n",
    "        # Attention mechanism parameters\n",
    "        self.attention = nn.Linear(hidden_size, hidden_size)  # Attention weights\n",
    "\n",
    "        # Initialize weights with a specific random seed\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Custom weight initialization for reproducibility\n",
    "        torch.manual_seed(42)  # Optional: redundant if already set globally\n",
    "\n",
    "        # Initialize fully connected layer weights with Xavier (Glorot) initialization\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        \n",
    "        # Initialize attention weights with Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.attention.weight)\n",
    "        \n",
    "        # Initialize biases to zero for fc layers\n",
    "        nn.init.constant_(self.fc1.bias, 0.0)\n",
    "        nn.init.constant_(self.fc2.bias, 0.0)\n",
    "        nn.init.constant_(self.fc3.bias, 0.0)\n",
    "        nn.init.constant_(self.attention.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        x = self.fc1(x)            # Input layer to first hidden layer\n",
    "        x = self.relu(x)           # ReLU activation\n",
    "        x = self.fc2(x)            # First hidden layer to second hidden layer\n",
    "        x = self.relu(x)           # ReLU activation\n",
    "\n",
    "        # Apply attention mechanism\n",
    "        attention_weights = self.attention(x)  # Attention mechanism\n",
    "        x = x * attention_weights  # Apply attention weights\n",
    "\n",
    "        x = self.fc3(x)            # Second hidden layer to output layer\n",
    "        return x\n",
    "\n",
    "model = SimpleKAN()\n",
    "i2_output = model(i_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52fc07c2-35b2-4284-bc45-072d39123138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save([i_output, o_output, a_output, i2_output], \"outputs.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
